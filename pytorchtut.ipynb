{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch # Tensor Package (for use on GPU)\n",
    "import torch.nn as nn ## Neural Network package\n",
    "import torch.nn.functional as F # Non-linearities package\n",
    "import torch.optim as optim # Optimization package\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader # for dealing with data\n",
    "import torchvision # for dealing with vision data\n",
    "import torchvision.transforms as transforms # for modifying vision data to run it through models\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Tensor Demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a one dimensional array the pytorch way (i.e. allowing GPU computations):\n",
    "x1 = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# here's a two dimensional array (i.e. of size 2 x 4):\n",
    "x2 = torch.tensor([[5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "\n",
    "# here's a three dimensional array (i.e. of size 2 x 2 x 4):\n",
    "x3 = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8]], [[9, 10, 11, 12], [13, 14, 15, 16]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x1[0])\n",
    "print(\"----------------------------------------\")\n",
    "# prints tensor(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single element from tensor is a zero dimension tensor and not a python primitive like in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x2[0, 0]) \n",
    "\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x2[0, :]) \n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x2[:, 2])\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x3\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x3[0, 0, 0]) \n",
    "# prints 1.0; the first entry of the first vector of the first set of vectors\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x3[:, 0, 0]) \n",
    "# prints 1, 9; the first entry of each first vector in each set of vectors\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x3[0, :, 0]) \n",
    "# prints 1, 5; pick the first set of vectors, and from each vector, choose the first entry\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x3[0, 0, :]) \n",
    "print(\"----------------------------------------\")\n",
    "# prints 1, 2, 3, 4; everything in the first vector of the first set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "# tensor with requires_grad set to True so we can use it for training and other stuff later\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(x)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(z, out)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "out.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)  Basic Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a single datapoint\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "# create a linear layer (i.e. a linear equation: w1x1 + w2x2 + w3x3 + w4x4 + b, with 4 inputs and 1 output)\n",
    "# w and b stand for weight and bias, respectively\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "print(\"----------------------------------------\")\n",
    "print(list(linear_layer1.named_parameters()))\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = linear_layer1(x)\n",
    "# run the datapoint x through the linear equation and put the output in predicted_y\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(predicted_y)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3)  Calculating The Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "\n",
    "# ideally, we want our model to predict 0 when we input our x1 variable below.\n",
    "target_y = torch.tensor([0.0])\n",
    "\n",
    "\n",
    "predicted_y = linear_layer1(x)\n",
    "print(\"----------------------------------------\")\n",
    "print(predicted_y)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss = loss_function(predicted_y, target_y)\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(loss)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4)  Recalculating/Updating Our Weights (using gradient of loss wrt weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Now, we're going to calculate the gradient of our loss function wrt our weights / biases\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "\n",
    "target_y = torch.tensor([0.0])\n",
    "\n",
    "predicted_y = linear_layer1(x)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss = loss_function(predicted_y, target_y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# now that we have computed the gradient, let's look at our weights before we change them:\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Weights (before update):\")\n",
    "print(linear_layer1.weight)\n",
    "print(linear_layer1.bias)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for f in linear_layer1.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "# we told the optimizer to subtract the learning rate * the gradient from our model weights\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Weights (after update):\")\n",
    "print(linear_layer1.weight)\n",
    "print(linear_layer1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pytorch's optimizer to auto update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Now, we're going to calculate the gradient of our loss function wrt our weights / biases\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "\n",
    "target_y = torch.tensor([0.0])\n",
    "\n",
    "predicted_y = linear_layer1(x)\n",
    "\n",
    "optimizer = optim.SGD(linear_layer1.parameters(), lr=0.1)\n",
    "# here we've created an optimizer object that's responsible for changing the weights\n",
    "# we told it which weights to change (those of our linear_layer1 model) and how much to change them (learning rate / lr)\n",
    "# but we haven't quite told it to change anything yet. First we have to calculate the gradient.\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss = loss_function(predicted_y, target_y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# now that we have computed the gradient, let's look at our weights before we change them:\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Weights (before update):\")\n",
    "print(linear_layer1.weight)\n",
    "print(linear_layer1.bias)\n",
    "\n",
    "optimizer.step()\n",
    "# we told the optimizer to subtract the learning rate * the gradient from our model weights\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Weights (after update):\")\n",
    "print(linear_layer1.weight)\n",
    "print(linear_layer1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5)  Updating Our Weights More Than Once (i.e. doing step 3-4 a few times aka \"epochs\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# this block of code is organized a little differently than section 4, but it's mostly the same code\n",
    "# the only three differences are:\n",
    "# - The \"Hyperparameter\" constants\n",
    "# - The for loop (for helping the model do <number of epochs> training steps)\n",
    "# - The linear_layer1.zero_grad() function call on line 25. \n",
    "#   (that's just to clear the gradients in memory, since we're starting the training over each iteration/epoch)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "target_y = torch.tensor([0.0])\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Output (BEFORE UPDATE):\")\n",
    "print(linear_layer1(x))\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3 # Number of times to update the weights\n",
    "LEARNING_RATE = 1e-2\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(linear_layer1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    linear_layer1.zero_grad() # Needed to clear the gradient buffer before each backprop\n",
    "    predicted_y = linear_layer1(x)\n",
    "    loss = loss_function(predicted_y, target_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Output (UPDATE \" + str(epoch + 1) + \"):\")\n",
    "    print(linear_layer1(x))\n",
    "    print(\"Should be getting closer to 0...\")\n",
    "\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6)  Making Our Epochs Only Use A Subset Of The Data (i.e. a \"minibatch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.Tensor([[0.0, 0.0, 1.0, 1.0],\n",
    "                 [0.0, 1.0, 1.0, 0.0],\n",
    "                 [1.0, 0.0, 1.0, 0.0],\n",
    "                 [1.0, 1.0, 1.0, 1.0]])\n",
    "target_y = torch.Tensor([0.0, 1.0, 1.0, 0.0])\n",
    "# now, instead of having 1 data sample, we have 4 (oh yea, now we're in the big leagues)\n",
    "# but, pytorch has a DataLoader class to help us scale up, so let's use that.\n",
    "\n",
    "inputs = x # let's use the same naming convention as the pytorch documentation here\n",
    "labels = target_y # and here\n",
    "\n",
    "train = TensorDataset(inputs, labels) # here we're just putting our data samples into a tiny Tensor dataset class\n",
    "\n",
    "trainloader = DataLoader(train, batch_size=2, shuffle=False) # and then putting the dataset above into a data loader\n",
    "# the batchsize=2 option just means that, later, when we iterate over it, we want to run our model on 2 samples at a time\n",
    "\n",
    "linear_layer1 = nn.Linear(4, 1)\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(linear_layer1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader): # here we split apart our data so we can run it\n",
    "        linear_layer1.zero_grad()\n",
    "        predicted_y = linear_layer1(inputs)\n",
    "        loss = loss_function(predicted_y, labels)\n",
    "#         loss = loss_function(predicted_y, labels.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Output (UPDATE: Epoch #\" + str(epoch + 1) + \", Batch #\" + str(batch_idx + 1) + \"):\")\n",
    "        print(linear_layer1(x))\n",
    "        print(\"Should be getting closer to [0, 1, 1, 0]...\") # but some of them aren't! we need a model that fits better...\n",
    "                                                             # next up, we'll convert this model from regression to a NN\n",
    "\n",
    "print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7)  Changing Our Model from Linear Regression to Neural Network (to make it fit the data better) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.Tensor([[0.0, 0.0, 1.0, 1.0],\n",
    "                 [0.0, 1.0, 1.0, 0.0],\n",
    "                 [1.0, 0.0, 1.0, 0.0],\n",
    "                 [1.0, 1.0, 1.0, 1.0]])\n",
    "target_y = torch.Tensor([0.0, 1.0, 1.0, 0.0])\n",
    "# now, instead of having 1 data sample, we have 4 (oh yea, now we're in the big leagues)\n",
    "# but, pytorch has a DataLoader class to help us scale up, so let's use that.\n",
    "\n",
    "inputs = x # let's use the same naming convention as the pytorch documentation here\n",
    "labels = target_y # and here\n",
    "\n",
    "train = TensorDataset(inputs, labels) # here we're just putting our data samples into a tiny Tensor dataset class\n",
    "\n",
    "trainloader = DataLoader(train, batch_size=2, shuffle=False) # and then putting the dataset above into a data loader\n",
    "# the batchsize=2 option just means that, later, when we iterate over it, we want to run our model on 2 samples at a time\n",
    "\n",
    "# Two layer Neural architecture\n",
    "linear_layer1 = nn.Linear(4, 2)\n",
    "sigmoid = nn.Sigmoid() # this is the nonlinearity that we pass the output from layers 1 and 2 into\n",
    "linear_layer2 = nn.Linear(2, 1) # this is our second layer (i.e. we're going to pass the outputs from sigmoid into here)\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 0.1\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(chain(linear_layer1.parameters(), linear_layer2.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        linear_layer1.zero_grad()\n",
    "        linear_layer2.zero_grad()\n",
    "        \n",
    "        linear_layer1_output = linear_layer1(inputs)\n",
    "        sigmoid_output = sigmoid(linear_layer1_output)\n",
    "        linear_layer2_output = linear_layer2(sigmoid_output)\n",
    "        sigmoid_output_2 = sigmoid(linear_layer2_output) # see how the output from one layer just goes into the second?\n",
    "        \n",
    "        loss = loss_function(sigmoid_output_2, labels.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Output (UPDATE: Epoch #\" + str(epoch + 1) + \", Batch #\" + str(batch_idx + 1) + \"):\")\n",
    "        print(sigmoid(linear_layer2(sigmoid(linear_layer1(x))))) # the nested functions are getting out of hand..\n",
    "        print(\"Should be getting closer to [0, 1, 1, 0]...\") # they are if you increase the epochs amount... but it's slow!\n",
    "\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8) Abstracting our Neural Network into its PyTorch class (i.e. making it more maintainable and less messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.Tensor([[0.0, 0.0, 1.0, 1.0],\n",
    "                 [0.0, 1.0, 1.0, 0.0],\n",
    "                 [1.0, 0.0, 1.0, 0.0],\n",
    "                 [1.0, 1.0, 1.0, 1.0]])\n",
    "target_y = torch.Tensor([0.0, 1.0, 1.0, 0.0])\n",
    "# now, instead of having 1 data sample, we have 4 (oh yea, now we're in the big leagues)\n",
    "# but, pytorch has a DataLoader class to help us scale up, so let's use that.\n",
    "\n",
    "inputs = x # let's use the same naming convention as the pytorch documentation here\n",
    "labels = target_y # and here\n",
    "\n",
    "train = TensorDataset(inputs, labels) # here we're just putting our data samples into a tiny Tensor dataset class\n",
    "\n",
    "trainloader = DataLoader(train, batch_size=2, shuffle=False) # and then putting the dataset above into a data loader\n",
    "# the batchsize=2 option just means that, later, when we iterate over it, we want to run our model on 2 samples at a time\n",
    "\n",
    "# Two layer Neural architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 2) # here's where we define the same layers we had earlier\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # the forward function just sends everything through its respective layers\n",
    "        x = sigmoid(x) # including through the sigmoids after each Linear layer\n",
    "        x = self.fc2(x)\n",
    "        x = sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = Net() # we made a blueprint above for our neural network, now we initialize one.\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 0.1\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "        \n",
    "        output = net(inputs) # but now, all we have to do is pass our inputs to the neural net \n",
    "        \n",
    "        loss = loss_function(output, labels.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Output (UPDATE: Epoch #\" + str(epoch + 1) + \", Batch #\" + str(batch_idx + 1) + \"):\")\n",
    "        print(sigmoid(linear_layer2(sigmoid(linear_layer1(x))))) # the nested functions are getting out of hand..\n",
    "        print(\"Should be getting closer to [0, 1, 1, 0]...\") # they are if you increase the epochs amount... but it's slow!\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Awesome, so we have a neural network (nn) in the actual PyTorch Net class.\n",
    "# As it stands right now, there's tons of optimization that can be done here.\n",
    "# But, at the risk of falling for premature optimization, let's get to the end and build our full-fledged CNN first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9)  Changing Our Input From Arbitrary Vectors To Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In preparation for building our Convolutional Neural Network (CNN), we're going to stop using random, arbitrary vectors.\n",
    "# Instead, we're going to use an actual standardized dataset: CIFAR-10\n",
    "# We also have built in modules to help us load/wrangle the dataset, so we're going to use those too! (since we're spoiled)\n",
    "\n",
    "transform = transforms.Compose( # we're going to use this to transform our data to make each sample more uniform\n",
    "   [\n",
    "    transforms.ToTensor(), # converts each sample from a (0-255, 0-255, 0-255) PIL Image format to a (0-1, 0-1, 0-1) FloatTensor format\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # for each of the 3 channels of the image, subtract mean 0.5 and divide by stdev 0.5\n",
    "   ]) # the normalization makes each SGD iteration more stable and overall makes convergence easier\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform) # this is all we need to get/wrangle the dataset!\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # each image can have 1 of 10 labels\n",
    "\n",
    "# helper function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've got a lot of boilerplate code out of the way, here's how it fits in to what we did above:\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 25) # now our first layer accepts inputs the size of the image's total information\n",
    "        self.fc2 = nn.Linear(25, 10) # Second layer has 25 hidden units\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3) # this just reshapes our tensor of image data so that we have <batch size>\n",
    "        x = self.fc1(x)             # in one dimension, and then the image data in the other\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "loss_function = nn.CrossEntropyLoss() # Changing our loss / cost function to work with our labels\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Awesome! Now it's trained. Time to test it:\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next() # just grabbing a sample from our test data set\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images)) # display the images we're going to predict\n",
    "\n",
    "outputs = net(images) # get our output from our neural network\n",
    "_, predicted = torch.max(outputs.data, 1) # get our predictions from the output\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "# print images\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "# and let's look at the overall accuracy:\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100.0 * correct / total))\n",
    "\n",
    "# Hmm, maybe we can do better. Let's add convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11)  Adding A Convolutional Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's all the boilerplate again:\n",
    "transform = transforms.Compose(\n",
    "   [\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "   ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # convolve each of our 3-channel images with 6 different 5x5 kernels, giving us 6 feature maps\n",
    "        self.fc1 = nn.Linear(4704, 120) # but that results in a 4x6x28x28 = 18816 dimensional output, 18816/4 = 4704 inputs per image.\n",
    "        self.fc2 = nn.Linear(120, 10) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.view(-1, 4704) # since our output from conv1 is 4x6x28x28, we need to flatten it into a 4x4704 (samples x features) tensor to feed it into a linear layer\n",
    "        x = self.fc1(x)             \n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holy guacamole, that takes a LOT longer. Those convolutions are expensive.\n",
    "# In the next section we'll make that a little quicker.\n",
    "# For now, let's see how much our predictions improved.\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100.0 * correct / total))\n",
    "\n",
    "# Okay... pretty good improvement. Again, before we prematurely optimize, let's add some pooling layers to make it quicker.\n",
    "# THEN we'll go ham on the optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12)  Adding A Pooling Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again, boilerplate:\n",
    "transform = transforms.Compose(\n",
    "   [\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "   ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # in any 2x2 square on each of our feature maps, take the most important (highest) one\n",
    "        self.fc1 = nn.Linear(1176, 120) # since we've pooled our outputs from the convolution, our input is reduced: 4704 -> 1176\n",
    "        self.fc2 = nn.Linear(120, 10) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid(x) # returns x of size: torch.Size([4, 6, 28, 28])\n",
    "        x = self.pool(x) # returns x of size: torch.Size([4, 6, 14, 14]) (so we have to adjust our linear input again)\n",
    "        x = x.view(-1, 1176) # now our input to the linear layer is going to be 4 by 6 * 14 * 14 = 1176\n",
    "        x = self.fc1(x)             \n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty significant speedup! Let's see how it affects accuracy:\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100.0 * correct / total))\n",
    "\n",
    "# Not by much! Awesome!\n",
    "# Now, let's add a few more layers, change our nonlinearities around, and do some other house keeping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13) Do Some Final Optimizations (i.e. making our first sigmoid a \"ReLU\",  and adding more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "   [\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "   ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5) # Let's add more feature maps - that might help\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5) # And another conv layer with even more feature maps\n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 120) # and finally, adjusting our first linear layer's input to our previous output\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) # we're changing our nonlinearity / activation function from sigmoid to ReLU for a slight speedup\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # after this pooling layer, we're down to a torch.Size([4, 20, 5, 5]) tensor.\n",
    "        x = x.view(-1, 20 * 5 * 5) # so let's adjust our tensor again.\n",
    "        x = self.fc1(x)             \n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# net = Net()\n",
    "net = Net().cuda() # Let's make our NN run on the GPU (I didn't splurge on this GTX 1080 for nothing...)\n",
    "\n",
    "NUMBER_OF_EPOCHS = 25 # Let's also increase our training cycles\n",
    "LEARNING_RATE = 1e-2 # And decrease our learning rate a little bit to compensate\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "#         inputs, labels = inputs.cuda(), labels.cuda() # Let's also make these tensors GPU compatible\n",
    "        output = net(inputs)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Iteration: \" + str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "outputs = net(images.cuda())\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    inputs = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Awesome! A lot better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14)  Bask... in the glory that is our newly created Convolutional Neural Network (CNN)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awesome - we have a full blown convolutional neural network!\n",
    "# Let's condense some stuff and put it all together without comments:\n",
    "\n",
    "transform = transforms.Compose(\n",
    "   [\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "   ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 5 * 5)           \n",
    "        x = F.relu(self.fc1(x) )\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net().cuda()\n",
    "\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "LEARNING_RATE = 1e-2\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        net.zero_grad()\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = net(inputs)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 5 is 0:\n",
    "        print(\"Iteration: \" + str(epoch + 1))\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "outputs = net(images.cuda())\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    labels = labels.cuda()\n",
    "    outputs = net(images.cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1597129145317"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}